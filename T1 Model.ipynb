{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6ebd29-35b8-42a8-8e47-e7bcdb5882b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN, Birch\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbbaab2-5479-4eb3-acf5-a2afb2292aae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "#     AWS Functions        #\n",
    "############################\n",
    "\n",
    "def boto3_session(flavor):\n",
    "  \"\"\"\n",
    "  boto3_session instantiates an AWS session. \n",
    "  \n",
    "  :flavor: indicates if the session should be created at the client or the resouce level\n",
    "  :return: session\n",
    "  \"\"\"\n",
    "\n",
    "  if flavor == 'client':\n",
    "    return(boto3.client('s3', 'us-east-2'))\n",
    "  else:\n",
    "    return(boto3.resource('s3', 'us-east-2'))\n",
    "\n",
    "\n",
    "def write_to_s3(sesh, key, obj):\n",
    "  \"\"\"\n",
    "  write_to_s3 will write textract formatted results to the artifact bucket using the provided key.\n",
    "  \n",
    "  :sesh: S3 session\n",
    "  :key: the prefix and key for the object in S3\n",
    "  \"\"\"\n",
    "  \n",
    "  sesh.Object('uwm-textract-910', key).put(Body=bytes(json.dumps(obj).encode('UTF-8')))\n",
    "  return\n",
    "\n",
    "def read_s3_subfiles_src(s3_sesh, prefix):\n",
    "  \"\"\"\n",
    "  read_s3_subfiles gathers all of the object keys from a specific directory in S3.\n",
    "  \n",
    "  :session: a boto3 resouce session\n",
    "  :folder: the directory for inspection\n",
    "  :return: an array of object keys\n",
    "  \"\"\"\n",
    "  bucket_src = s3_sesh.Bucket('uwm-textract-910')\n",
    "  files = []\n",
    "  \n",
    "\n",
    "  for object_summary in bucket_src.objects.filter(Prefix=prefix):\n",
    "    if object_summary.key.endswith('.json'):\n",
    "        files.append(object_summary.key)\n",
    "\n",
    "    #remove for live automation\n",
    "    # if len(files) == 1000:\n",
    "    #     break\n",
    "  \n",
    "  return files\n",
    "\n",
    "def retrieve_s3_subfile (s3_sesh, file):\n",
    "  \n",
    "  con_obj = s3_sesh.Object('uwm-textract-910', file)\n",
    "  file_content = con_obj.get()['Body'].read().decode('utf-8')\n",
    "  json_content = json.loads(file_content)\n",
    "  \n",
    "  return json_content\n",
    "\n",
    "############################\n",
    "#       METADATA           #\n",
    "############################\n",
    "\n",
    "def append_height_diff_above(json_page):\n",
    "    for i in range(len(json_page['Blocks']) - 1):\n",
    "        previous_line = i\n",
    "        current_line = i + 1\n",
    "            \n",
    "        previous_font_size = json_page['Blocks'][previous_line]['BoundingBox']['Height']\n",
    "        current_t = json_page['Blocks'][current_line]['BoundingBox']['Top']\n",
    "        previous_t = json_page['Blocks'][previous_line]['BoundingBox']['Top']\n",
    "        height_diff = current_t - (previous_t + previous_font_size)\n",
    "        # ive tested with absolute value but do we want the possibility of negative differences?\n",
    "        json_page['Blocks'][current_line]['height_diff_above'] = abs(height_diff)\n",
    "\n",
    "    json_page['Blocks'][0]['height_diff_above'] = 0\n",
    "    return json_page\n",
    "  \n",
    "  \n",
    "def append_height_diff_below(json_page):\n",
    "    for i in range(len(json_page['Blocks']) - 1):\n",
    "        previous_line = i\n",
    "        current_line = i + 1\n",
    "            \n",
    "        previous_font_size = json_page['Blocks'][previous_line]['BoundingBox']['Height']\n",
    "        current_t = json_page['Blocks'][current_line]['BoundingBox']['Top']\n",
    "        previous_t = json_page['Blocks'][previous_line]['BoundingBox']['Top']\n",
    "        height_diff = current_t - (previous_t + previous_font_size)\n",
    "        # ive tested with absolute value but do we want the possibility of negative differences?\n",
    "        json_page['Blocks'][previous_line]['height_diff_below'] = abs(height_diff)\n",
    "\n",
    "    json_page['Blocks'][-1]['height_diff_below'] = 0\n",
    "    return json_page\n",
    "\n",
    "  \n",
    "def append_paragraph_number(json_page):\n",
    "    blocks = json_page['Blocks']\n",
    "    precision = 2\n",
    "    p_count = 0\n",
    "    for i in range(len(json_page['Blocks'])):\n",
    "        below = round(json_page['Blocks'][i]['height_diff_below'],precision)\n",
    "        above = round(json_page['Blocks'][i]['height_diff_above'],precision)\n",
    "        if above > below:\n",
    "            p_count +=1\n",
    "        json_page['Blocks'][i]['paragraph'] = p_count\n",
    "    return json_page\n",
    "\n",
    "  \n",
    "def append_paragraph_number_std(json_page):\n",
    "    blocks = json_page['Blocks']\n",
    "    data = pd.json_normalize(blocks)\n",
    "    p_counter = 0 \n",
    "    p_counts = []\n",
    "    # remove first observation from calculation since it\n",
    "    # has a zero in it since first line doesnt have a height above\n",
    "    height_diff_std = data.height_diff_above.iloc[1::].std()\n",
    "    height_diff_mean = data.height_diff_above.iloc[1::].mean()\n",
    "    json_page['height_diff_mean'] = height_diff_mean\n",
    "    json_page['height_diff_std'] = height_diff_std\n",
    "    \n",
    "    for i in range(len(json_page['Blocks'])):\n",
    "        if json_page['Blocks'][i]['height_diff_above'] > (height_diff_mean+height_diff_std):\n",
    "            p_counter+=1\n",
    "        json_page['Blocks'][i]['paragraph_std'] = p_counter\n",
    "    return json_page\n",
    "      \n",
    "      \n",
    "def append_scaled(json_page):\n",
    "    blocks = json_page['Blocks']\n",
    "    data = pd.json_normalize(blocks)\n",
    "    feats = ['BoundingBox.Top', 'BoundingBox.Left', 'paragraph_std', 'paragraph']\n",
    "    X = data[feats]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scl = scaler.fit_transform(X)\n",
    "    dim1 = X_scl.shape[0]\n",
    "    dim2 = X_scl.shape[-1]\n",
    "    for i in range(dim1):\n",
    "        for j in range(dim2):\n",
    "            if j==0:\n",
    "                json_page['Blocks'][i]['top_scl'] = X_scl[i][j]\n",
    "            if j == 1:\n",
    "                json_page['Blocks'][i]['left_scl'] = X_scl[i][j]\n",
    "            elif j == 2:\n",
    "                json_page['Blocks'][i]['paragraph_std_scl'] = X_scl[i][j]\n",
    "            else:\n",
    "                json_page['Blocks'][i]['paragraph_scl'] = X_scl[i][j]\n",
    "    return json_page\n",
    "        \n",
    "\n",
    "def append_cluster(json_page, pca=True):\n",
    "    blocks = json_page['Blocks']\n",
    "    feats = ['top_scl', 'left_scl', 'paragraph_std_scl']\n",
    "    X_scl = pd.json_normalize(blocks)[feats]\n",
    "\n",
    "    if pca:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_out = pca.fit_transform(X_scl)\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=2)\n",
    "        clusterer.fit(pca_out)\n",
    "        hdbscan_labels = clusterer.labels_\n",
    "        cluster_probs = clusterer.probabilities_\n",
    "    else:\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=2)\n",
    "        clusterer.fit(pca_out)\n",
    "        hdbscan_labels = clusterer.labels_\n",
    "        cluster_probs = clusterer.probabilities_\n",
    "\n",
    "\n",
    "    for i in range(len(json_page['Blocks'])):\n",
    "        json_page['Blocks'][i]['text_cluster'] = int(hdbscan_labels[i])\n",
    "        json_page['Blocks'][i]['cluster_probability'] = float(cluster_probs[i])\n",
    "\n",
    "    return json_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c4c7a7-0886-4272-a9b2-888010adaf1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "#       MAIN               #\n",
    "############################\n",
    "\n",
    "def main():\n",
    "    file_arr = []\n",
    "\n",
    "    s3_sesh = boto3_session('resource')\n",
    "    target = f'Textract_Output'\n",
    "  \n",
    "    #for target in targets\n",
    "    #gather the files paths from S3\n",
    "    files = read_s3_subfiles_src(s3_sesh, target)\n",
    "    print(files)\n",
    "    \n",
    "    #gather the file contents \n",
    "    for idx, file in enumerate(files):\n",
    "\n",
    "      page = retrieve_s3_subfile(s3_sesh, file)\n",
    "      file_name = file.split('/')[-1].split('.')[0]+'.pdf'\n",
    "      dest_name = file.split('/')[-1]\n",
    "      dest = f'T1_Model/{dest_name}'\n",
    "\n",
    "      if len(page['Blocks']) > 1:\n",
    "          height_diff_above_json = append_height_diff_above(page)\n",
    "          height_diff_below_json = append_height_diff_below(height_diff_above_json)\n",
    "          paragraph_number_json = append_paragraph_number(height_diff_below_json)\n",
    "          pg_num_std_json = append_paragraph_number_std(paragraph_number_json)\n",
    "          scaled_json = append_scaled(pg_num_std_json)\n",
    "          clustered_json = append_cluster(scaled_json)\n",
    "      elif len(page['Blocks']) > 0:\n",
    "          clustered_json = page\n",
    "          clustered_json['Blocks'][0]['height_diff_above'] = None\n",
    "          clustered_json['Blocks'][0]['height_diff_below'] = None\n",
    "          clustered_json['Blocks'][0]['paragraph'] = None\n",
    "          clustered_json['Blocks'][0]['paragraph_std'] = None\n",
    "          clustered_json['Blocks'][0]['top_scl'] = None\n",
    "          clustered_json['Blocks'][0]['paragraph_scl'] = None\n",
    "          clustered_json['Blocks'][0]['left_scl'] = None\n",
    "          clustered_json['Blocks'][0]['paragraph_std_scl'] = None\n",
    "          clustered_json['Blocks'][0]['text_cluster'] = 1\n",
    "          clustered_json['Blocks'][0]['cluster_probability'] = 1\n",
    "          clustered_json['height_diff_mean'] = None\n",
    "          clustered_json['height_diff_std'] = None \n",
    "\n",
    "\n",
    "      clustered_json['file_name'] = file_name\n",
    "      \n",
    "      \n",
    "\n",
    "    #   file_dest = f'{dest}/{uuid}/{file_name}/{page_num}'\n",
    "      write_to_s3(s3_sesh, dest, clustered_json)\n",
    "      file_arr.append(dest)\n",
    "\n",
    "    return file_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741535f7-ff55-4266-9faa-7c5433f6df11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Textract_Output/Sample 1.json', 'Textract_Output/Sample 10.json', 'Textract_Output/Sample 11.json', 'Textract_Output/Sample 12.json', 'Textract_Output/Sample 13.json', 'Textract_Output/Sample 14.json', 'Textract_Output/Sample 15.json', 'Textract_Output/Sample 16.json', 'Textract_Output/Sample 17.json', 'Textract_Output/Sample 18.json', 'Textract_Output/Sample 19.json', 'Textract_Output/Sample 2.json', 'Textract_Output/Sample 20.json', 'Textract_Output/Sample 21.json', 'Textract_Output/Sample 22.json', 'Textract_Output/Sample 23.json', 'Textract_Output/Sample 24.json', 'Textract_Output/Sample 25.json', 'Textract_Output/Sample 26.json', 'Textract_Output/Sample 27.json', 'Textract_Output/Sample 28.json', 'Textract_Output/Sample 29.json', 'Textract_Output/Sample 3.json', 'Textract_Output/Sample 30.json', 'Textract_Output/Sample 31.json', 'Textract_Output/Sample 32.json', 'Textract_Output/Sample 33.json', 'Textract_Output/Sample 34.json', 'Textract_Output/Sample 35.json', 'Textract_Output/Sample 36.json', 'Textract_Output/Sample 37.json', 'Textract_Output/Sample 38.json', 'Textract_Output/Sample 39.json', 'Textract_Output/Sample 4.json', 'Textract_Output/Sample 40.json', 'Textract_Output/Sample 41.json', 'Textract_Output/Sample 42.json', 'Textract_Output/Sample 43.json', 'Textract_Output/Sample 44.json', 'Textract_Output/Sample 45.json', 'Textract_Output/Sample 46.json', 'Textract_Output/Sample 47.json', 'Textract_Output/Sample 48.json', 'Textract_Output/Sample 49.json', 'Textract_Output/Sample 5.json', 'Textract_Output/Sample 50.json', 'Textract_Output/Sample 51.json', 'Textract_Output/Sample 52.json', 'Textract_Output/Sample 53.json', 'Textract_Output/Sample 54.json', 'Textract_Output/Sample 55.json', 'Textract_Output/Sample 56.json', 'Textract_Output/Sample 57.json', 'Textract_Output/Sample 58.json', 'Textract_Output/Sample 59.json', 'Textract_Output/Sample 6.json', 'Textract_Output/Sample 60.json', 'Textract_Output/Sample 7.json', 'Textract_Output/Sample 8.json', 'Textract_Output/Sample 9.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['T1_Model/Sample 1.json',\n",
       " 'T1_Model/Sample 10.json',\n",
       " 'T1_Model/Sample 11.json',\n",
       " 'T1_Model/Sample 12.json',\n",
       " 'T1_Model/Sample 13.json',\n",
       " 'T1_Model/Sample 14.json',\n",
       " 'T1_Model/Sample 15.json',\n",
       " 'T1_Model/Sample 16.json',\n",
       " 'T1_Model/Sample 17.json',\n",
       " 'T1_Model/Sample 18.json',\n",
       " 'T1_Model/Sample 19.json',\n",
       " 'T1_Model/Sample 2.json',\n",
       " 'T1_Model/Sample 20.json',\n",
       " 'T1_Model/Sample 21.json',\n",
       " 'T1_Model/Sample 22.json',\n",
       " 'T1_Model/Sample 23.json',\n",
       " 'T1_Model/Sample 24.json',\n",
       " 'T1_Model/Sample 25.json',\n",
       " 'T1_Model/Sample 26.json',\n",
       " 'T1_Model/Sample 27.json',\n",
       " 'T1_Model/Sample 28.json',\n",
       " 'T1_Model/Sample 29.json',\n",
       " 'T1_Model/Sample 3.json',\n",
       " 'T1_Model/Sample 30.json',\n",
       " 'T1_Model/Sample 31.json',\n",
       " 'T1_Model/Sample 32.json',\n",
       " 'T1_Model/Sample 33.json',\n",
       " 'T1_Model/Sample 34.json',\n",
       " 'T1_Model/Sample 35.json',\n",
       " 'T1_Model/Sample 36.json',\n",
       " 'T1_Model/Sample 37.json',\n",
       " 'T1_Model/Sample 38.json',\n",
       " 'T1_Model/Sample 39.json',\n",
       " 'T1_Model/Sample 4.json',\n",
       " 'T1_Model/Sample 40.json',\n",
       " 'T1_Model/Sample 41.json',\n",
       " 'T1_Model/Sample 42.json',\n",
       " 'T1_Model/Sample 43.json',\n",
       " 'T1_Model/Sample 44.json',\n",
       " 'T1_Model/Sample 45.json',\n",
       " 'T1_Model/Sample 46.json',\n",
       " 'T1_Model/Sample 47.json',\n",
       " 'T1_Model/Sample 48.json',\n",
       " 'T1_Model/Sample 49.json',\n",
       " 'T1_Model/Sample 5.json',\n",
       " 'T1_Model/Sample 50.json',\n",
       " 'T1_Model/Sample 51.json',\n",
       " 'T1_Model/Sample 52.json',\n",
       " 'T1_Model/Sample 53.json',\n",
       " 'T1_Model/Sample 54.json',\n",
       " 'T1_Model/Sample 55.json',\n",
       " 'T1_Model/Sample 56.json',\n",
       " 'T1_Model/Sample 57.json',\n",
       " 'T1_Model/Sample 58.json',\n",
       " 'T1_Model/Sample 59.json',\n",
       " 'T1_Model/Sample 6.json',\n",
       " 'T1_Model/Sample 60.json',\n",
       " 'T1_Model/Sample 7.json',\n",
       " 'T1_Model/Sample 8.json',\n",
       " 'T1_Model/Sample 9.json']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_arr = main()\n",
    "file_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve File Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T1_Model/Sample 1.json',\n",
       " 'T1_Model/Sample 10.json',\n",
       " 'T1_Model/Sample 11.json',\n",
       " 'T1_Model/Sample 12.json',\n",
       " 'T1_Model/Sample 13.json',\n",
       " 'T1_Model/Sample 14.json',\n",
       " 'T1_Model/Sample 15.json',\n",
       " 'T1_Model/Sample 16.json',\n",
       " 'T1_Model/Sample 17.json',\n",
       " 'T1_Model/Sample 18.json',\n",
       " 'T1_Model/Sample 19.json',\n",
       " 'T1_Model/Sample 2.json',\n",
       " 'T1_Model/Sample 20.json',\n",
       " 'T1_Model/Sample 21.json',\n",
       " 'T1_Model/Sample 22.json',\n",
       " 'T1_Model/Sample 23.json',\n",
       " 'T1_Model/Sample 24.json',\n",
       " 'T1_Model/Sample 25.json',\n",
       " 'T1_Model/Sample 26.json',\n",
       " 'T1_Model/Sample 27.json',\n",
       " 'T1_Model/Sample 28.json',\n",
       " 'T1_Model/Sample 29.json',\n",
       " 'T1_Model/Sample 3.json',\n",
       " 'T1_Model/Sample 30.json',\n",
       " 'T1_Model/Sample 31.json',\n",
       " 'T1_Model/Sample 32.json',\n",
       " 'T1_Model/Sample 33.json',\n",
       " 'T1_Model/Sample 34.json',\n",
       " 'T1_Model/Sample 35.json',\n",
       " 'T1_Model/Sample 36.json',\n",
       " 'T1_Model/Sample 37.json',\n",
       " 'T1_Model/Sample 38.json',\n",
       " 'T1_Model/Sample 39.json',\n",
       " 'T1_Model/Sample 4.json',\n",
       " 'T1_Model/Sample 40.json',\n",
       " 'T1_Model/Sample 41.json',\n",
       " 'T1_Model/Sample 42.json',\n",
       " 'T1_Model/Sample 43.json',\n",
       " 'T1_Model/Sample 44.json',\n",
       " 'T1_Model/Sample 45.json',\n",
       " 'T1_Model/Sample 46.json',\n",
       " 'T1_Model/Sample 47.json',\n",
       " 'T1_Model/Sample 48.json',\n",
       " 'T1_Model/Sample 49.json',\n",
       " 'T1_Model/Sample 5.json',\n",
       " 'T1_Model/Sample 50.json',\n",
       " 'T1_Model/Sample 51.json',\n",
       " 'T1_Model/Sample 52.json',\n",
       " 'T1_Model/Sample 53.json',\n",
       " 'T1_Model/Sample 54.json',\n",
       " 'T1_Model/Sample 55.json',\n",
       " 'T1_Model/Sample 56.json',\n",
       " 'T1_Model/Sample 57.json',\n",
       " 'T1_Model/Sample 58.json',\n",
       " 'T1_Model/Sample 59.json',\n",
       " 'T1_Model/Sample 6.json',\n",
       " 'T1_Model/Sample 60.json',\n",
       " 'T1_Model/Sample 7.json',\n",
       " 'T1_Model/Sample 8.json',\n",
       " 'T1_Model/Sample 9.json']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3_session('resouce')\n",
    "res_files = read_s3_subfiles_src(s3, 'T1_Model')\n",
    "res_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in res_files: \n",
    "    file_name = file.split('/')[-1]\n",
    "    j_obj = retrieve_s3_subfile(s3, file)\n",
    "\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(j_obj, indent=4)\n",
    "    \n",
    "    # Writing to sample.json\n",
    "    with open(f'Export Data/T1 Model Results/{file_name}', \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MMCS Paragraph Clustering",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
